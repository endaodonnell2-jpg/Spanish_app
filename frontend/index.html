<!DOCTYPE html>
<html lang="en">
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Walkie-Talkie Mobile</title>
<style>
  body {
    margin: 0;
    font-family: sans-serif;
    display: flex;
    flex-direction: column;
    height: 100vh;
    background: #f5f1e6; /* light beige */
    color: #000;
    overflow: hidden;
  }

  /* Single horizontal sound bar */
  #soundBar {
    height: 14px;
    margin: 10px 10px 0 10px;
    border-radius: 7px;
    background: #ccc;
    transition: width 0.05s, background 0.05s;
  }

  /* Text display (last sentences only) */
  #textArea {
    flex: 1;
    display: flex;
    flex-direction: column;
    justify-content: flex-start;
    padding: 5px 10px;
    font-size: 1rem;
    overflow: hidden;
  }

  .userText { color: #d9534f; margin: 2px 0; }
  .aiText { color: #5cb85c; margin: 2px 0; }

  /* Hold to talk button */
  #talkBtn {
    position: absolute;
    bottom: 12px; /* slightly above base */
    left: 50%;
    transform: translateX(-50%);
    width: 80%;
    height: 50px;
    border: none;
    border-radius: 12px;
    font-size: 1.2rem;
    background: #5cb85c;
    color: #fff;
    text-align: center;
    transition: background 0.2s, transform 0.1s;
    display: flex;
    justify-content: center;
    align-items: center;
    z-index: 10;
  }

  #talkBtn.active {
    background: #d9534f;
    transform: scale(1.05);
  }
</style>
</head>
<body>

<!-- Single horizontal sound bar -->
<div id="soundBar"></div>

<!-- Text area -->
<div id="textArea">
  <div class="userText" id="lastUser"></div>
  <div class="aiText" id="lastAI"></div>
</div>

<!-- Hold-to-talk button -->
<button id="talkBtn">Hold</button>

<script>
const talkBtn = document.getElementById('talkBtn');
const soundBar = document.getElementById('soundBar');
const lastUser = document.getElementById('lastUser');
const lastAI = document.getElementById('lastAI');

let mediaRecorder, audioChunks = [];
let isRecording = false;
let userStream;

// Animate bar by volume
function animateBar(volume, speaker) {
  soundBar.style.width = `${Math.min(10 + volume * 90, 100)}%`;
  if(speaker === 'user') soundBar.style.background = '#d9534f';
  else if(speaker === 'ai') soundBar.style.background = '#5cb85c';
  else soundBar.style.background = '#ccc';
}

// Start recording
async function startRecording() {
  isRecording = true;
  talkBtn.classList.add('active');
  lastUser.textContent = '...';

  userStream = await navigator.mediaDevices.getUserMedia({ audio: true });
  mediaRecorder = new MediaRecorder(userStream);
  audioChunks = [];
  mediaRecorder.ondataavailable = e => audioChunks.push(e.data);
  mediaRecorder.start();

  // Microphone volume analysis
  const audioContext = new AudioContext();
  const source = audioContext.createMediaStreamSource(userStream);
  const analyser = audioContext.createAnalyser();
  analyser.fftSize = 256;
  source.connect(analyser);
  const dataArray = new Uint8Array(analyser.frequencyBinCount);

  function updateUserBar() {
    if(!isRecording) return;
    analyser.getByteFrequencyData(dataArray);
    const avg = dataArray.reduce((a,b)=>a+b,0)/dataArray.length/255;
    animateBar(avg, avg > 0.02 ? 'user' : null);
    requestAnimationFrame(updateUserBar);
  }
  updateUserBar();
}

// Play TTS with real amplitude
function playAudioWithBar(url, aiText) {
  const audio = new Audio(url);
  lastAI.textContent = '...';
  const audioContext = new AudioContext();
  const source = audioContext.createMediaElementSource(audio);
  const analyser = audioContext.createAnalyser();
  analyser.fftSize = 256;
  source.connect(analyser);
  analyser.connect(audioContext.destination);
  const dataArray = new Uint8Array(analyser.frequencyBinCount);

  function updateAIBar() {
    analyser.getByteFrequencyData(dataArray);
    const avg = dataArray.reduce((a,b)=>a+b,0)/dataArray.length/255;
    animateBar(avg, avg > 0.01 ? 'ai' : null);
    if(!audio.paused) requestAnimationFrame(updateAIBar);
  }

  audio.onplay = () => { updateAIBar(); }
  audio.onended = () => {
    soundBar.style.width = '0%';
    soundBar.style.background = '#ccc';
    lastAI.textContent = aiText;
  }
  audio.play();
}

// Stop recording and send
async function stopRecording() {
  if(!mediaRecorder) return;
  isRecording = false;
  talkBtn.classList.remove('active');
  soundBar.style.width = '0%';
  soundBar.style.background = '#ccc';
  mediaRecorder.stop();

  mediaRecorder.onstop = async () => {
    const blob = new Blob(audioChunks, { type: 'audio/webm' });
    const formData = new FormData();
    formData.append('file', blob, 'talk.webm');

    try {
      const response = await fetch('/process_audio', { method: 'POST', body: formData });
      const data = await response.json();
      lastUser.textContent = data.user_text; // last user sentence
      playAudioWithBar(data.tts_url, data.ai_text); // last AI sentence
    } catch(e) {
      console.error('Error processing audio', e);
      alert('Failed to process audio');
    }

    if(userStream) userStream.getTracks().forEach(track => track.stop());
  }
}

// Button events
talkBtn.addEventListener('mousedown', startRecording);
talkBtn.addEventListener('mouseup', stopRecording);
talkBtn.addEventListener('touchstart', e => { e.preventDefault(); startRecording(); });
talkBtn.addEventListener('touchend', e => { e.preventDefault(); stopRecording(); });
</script>
</body>
</html>
