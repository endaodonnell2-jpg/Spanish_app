<script>
const canvas = document.getElementById("visualizer");
const ctx = canvas.getContext("2d");
const micBtn = document.getElementById("micBtn");

let audioCtx;
let analyser;
let dataArray;
let mediaRecorder;
let audioChunks = [];
let state = "idle";
let pressStartTime = 0;

let currentAudio = null;
let currentSource = null;
let currentController = null;
let latestRequestId = 0;

/* ---------------- AUDIO CONTEXT ---------------- */

async function ensureAudioRunning() {
    if (!audioCtx) {
        audioCtx = new (window.AudioContext || window.webkitAudioContext)();
    }
    if (audioCtx.state === "suspended") {
        await audioCtx.resume();
    }
}

/* Fix mobile suspension */
window.addEventListener("pointerdown", ensureAudioRunning);
window.addEventListener("touchstart", ensureAudioRunning);

/* ---------------- CLEANUP ---------------- */

function stopCurrentAudio() {
    if (currentAudio) {
        currentAudio.pause();
        currentAudio.src = "";
        currentAudio.load();
        currentAudio = null;
    }
    if (currentSource) {
        try { currentSource.disconnect(); } catch {}
        currentSource = null;
    }
}

function cancelRequest() {
    if (currentController) {
        currentController.abort();
        currentController = null;
    }
}

/* ---------------- CANVAS ---------------- */

function resizeCanvas() {
    canvas.width = canvas.offsetWidth;
    canvas.height = canvas.offsetHeight;
}
resizeCanvas();
window.addEventListener("resize", resizeCanvas);

function startVisualizer(sourceNode) {
    analyser = audioCtx.createAnalyser();
    analyser.fftSize = 2048;
    sourceNode.connect(analyser);

    const bufferLength = analyser.fftSize;
    dataArray = new Uint8Array(bufferLength);

    function draw() {
        requestAnimationFrame(draw);
        ctx.clearRect(0, 0, canvas.width, canvas.height);

        if (state === "idle" || state === "thinking") {
            drawFlatLine();
            return;
        }

        analyser.getByteTimeDomainData(dataArray);

        ctx.lineWidth = 3;
        ctx.strokeStyle = state === "user" ? "#e74c3c" : "#2ecc71";
        ctx.beginPath();

        const sliceWidth = canvas.width / bufferLength;
        let x = 0;

        for (let i = 0; i < bufferLength; i++) {
            const v = dataArray[i] / 128.0;
            const y = v * canvas.height / 2;
            if (i === 0) ctx.moveTo(x, y);
            else ctx.lineTo(x, y);
            x += sliceWidth;
        }

        ctx.lineTo(canvas.width, canvas.height / 2);
        ctx.stroke();
    }

    draw();
}

function drawFlatLine() {
    ctx.lineWidth = 2;
    ctx.strokeStyle = "#bbbbbb";
    ctx.beginPath();
    ctx.moveTo(0, canvas.height / 2);
    ctx.lineTo(canvas.width, canvas.height / 2);
    ctx.stroke();
}

/* ---------------- RECORDING ---------------- */

micBtn.addEventListener("pointerdown", async (e) => {
    e.stopPropagation();
    await ensureAudioRunning();

    // HARD INTERRUPT EVERYTHING
    stopCurrentAudio();
    cancelRequest();

    state = "user";
    micBtn.classList.add("active", "pressed");
    pressStartTime = performance.now();
    audioChunks = [];

    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    const source = audioCtx.createMediaStreamSource(stream);
    startVisualizer(source);

    mediaRecorder = new MediaRecorder(stream);
    mediaRecorder.ondataavailable = e => audioChunks.push(e.data);
    mediaRecorder.onstop = sendAudio;
    mediaRecorder.start();
});

function stopRecording() {
    const pressDuration = performance.now() - pressStartTime;

    if (mediaRecorder && mediaRecorder.state === "recording") {
        mediaRecorder.stop();
    }

    micBtn.classList.remove("pressed");

    if (pressDuration < 200) {
        audioChunks = [];
        micBtn.classList.remove("active");
        state = "idle";
        return;
    }

    micBtn.classList.add("pop");
    setTimeout(() => micBtn.classList.remove("pop"), 250);

    micBtn.classList.remove("active");
    state = "thinking";
    micBtn.classList.add("thinking");
}

window.addEventListener("pointerup", stopRecording);
window.addEventListener("pointercancel", stopRecording);
window.addEventListener("pointerleave", stopRecording);

/* ---------------- SEND TO BACKEND ---------------- */

async function sendAudio() {
    const blob = new Blob(audioChunks, { type: "audio/webm" });

    if (blob.size < 1000) {
        state = "idle";
        micBtn.classList.remove("thinking");
        return;
    }

    const formData = new FormData();
    formData.append("file", blob, "user.webm");

    const requestId = ++latestRequestId;
    currentController = new AbortController();

    try {
        const response = await fetch("/process_audio", {
            method: "POST",
            body: formData,
            signal: currentController.signal
        });

        const data = await response.json();

        if (requestId === latestRequestId && data.tts_url) {
            playAI(data.tts_url);
        }

    } catch (err) {
        if (err.name !== "AbortError") {
            console.error("Request error:", err);
        }
    }
}

/* ---------------- AI PLAYBACK ---------------- */

function playAI(url) {
    stopCurrentAudio();

    state = "ai";
    micBtn.classList.remove("thinking");

    const audio = new Audio(url + "?t=" + Date.now());
    audio.crossOrigin = "anonymous";
    currentAudio = audio;

    const source = audioCtx.createMediaElementSource(audio);
    currentSource = source;

    startVisualizer(source);
    source.connect(audioCtx.destination);

    audio.onended = () => {
        state = "idle";
    };

    audio.play();
}

drawFlatLine();
</script>
