<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>Lucas11 Mobile</title>
    <style>
        :root { --neutral: #2ecc71; --active: #e74c3c; --bg: #121212; }
        
        * { box-sizing: border-box; }

        body { 
            margin: 0; padding: 0; background: var(--bg); color: white;
            font-family: -apple-system, system-ui, sans-serif;
            display: flex; flex-direction: column; 
            height: 100vh; height: 100dvh; /* Fixes mobile screen cutoff */
            overflow: hidden;
        }

        #visualizer-container {
            height: 20%; width: 100%; display: flex; align-items: center; 
            justify-content: center; background: #1a1a1a;
        }
        canvas { width: 90%; height: 60px; }

        #transcript-box {
            flex: 1; padding: 20px; display: flex; align-items: center; 
            justify-content: center; text-align: center; font-size: 1.2rem;
        }

        #controls { 
            padding: 20px; 
            padding-bottom: env(safe-area-inset-bottom); /* Fixes iPhone notch issue */
        }

        #holdBtn {
            width: 100%; height: 80px; border-radius: 15px; border: none;
            background: var(--neutral); color: white; font-size: 1.5rem; font-weight: bold;
            touch-action: none; 
        }
        #holdBtn.active { background: var(--active); }
    </style>
</head>
<body>

    <div id="visualizer-container">
        <canvas id="visualizer"></canvas>
    </div>

    <div id="transcript-box">
        <p id="last-text">Hold to talk to Lucas11</p>
    </div>

    <div id="controls">
        <button id="holdBtn">HOLD TO SPEAK</button>
    </div>

<script>
    const holdBtn = document.getElementById('holdBtn');
    const transcriptText = document.getElementById('last-text');
    const canvas = document.getElementById('visualizer');
    const canvasCtx = canvas.getContext('2d');

    let audioCtx, analyser, dataArray, source, mediaRecorder;
    let audioChunks = [];

    async function initAudio() {
        if (!audioCtx) {
            audioCtx = new (window.AudioContext || window.webkitAudioContext)();
        }
        if (audioCtx.state === 'suspended') {
            await audioCtx.resume();
        }
    }

    function setupVisualizer(streamSource, color) {
        if (!analyser) {
            analyser = audioCtx.createAnalyser();
            analyser.fftSize = 256;
        }
        streamSource.connect(analyser);
        const bufferLength = analyser.frequencyBinCount;
        dataArray = new Uint8Array(bufferLength);

        function draw() {
            requestAnimationFrame(draw);
            analyser.getByteFrequencyData(dataArray);
            canvasCtx.clearRect(0, 0, canvas.width, canvas.height);
            canvasCtx.fillStyle = color;
            let x = 0;
            for(let i = 0; i < bufferLength; i++) {
                const barHeight = dataArray[i] / 2;
                canvasCtx.fillRect(x, canvas.height - barHeight, 2, barHeight);
                x += 3;
            }
        }
        draw();
    }

    holdBtn.addEventListener('pointerdown', async (e) => {
        e.preventDefault();
        await initAudio(); // CRITICAL: Unlocks audio on mobile
        holdBtn.classList.add('active');
        audioChunks = [];
        
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        const micSource = audioCtx.createMediaStreamSource(stream);
        setupVisualizer(micSource, '#e74c3c');
        
        mediaRecorder = new MediaRecorder(stream);
        mediaRecorder.ondataavailable = (e) => audioChunks.push(e.data);
        mediaRecorder.onstop = sendAudio;
        mediaRecorder.start();
    });

    window.addEventListener('pointerup', () => {
        if (mediaRecorder && mediaRecorder.state === "recording") {
            mediaRecorder.stop();
            holdBtn.classList.remove('active');
        }
    });

    async function sendAudio() {
        const blob = new Blob(audioChunks, { type: 'audio/webm' });
        if (blob.size < 1000) return;

        const formData = new FormData();
        formData.append('file', blob, 'user.webm');
        transcriptText.innerText = "Thinking...";

        const response = await fetch('/process_audio', { method: 'POST', body: formData });
        const data = await response.json();
        if (data.tts_url) playAI(data.tts_url);
    }

    function playAI(url) {
        const audio = new Audio(url);
        audio.crossOrigin = "anonymous";
        const aiSource = audioCtx.createMediaElementSource(audio);
        setupVisualizer(aiSource, '#2ecc71');
        aiSource.connect(audioCtx.destination);
        audio.play();
        transcriptText.innerText = "Lucas11 is speaking...";
    }
</script>
</body>
</html>
